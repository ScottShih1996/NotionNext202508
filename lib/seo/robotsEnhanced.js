/**
 * 增强版robots.txt生成器
 * 提供更详细的爬虫控制和SEO优化
 */

import { siteConfig } from '@/lib/config'
import fs from 'fs'

/**
 * 生成增强版robots.txt
 * @param {Object} props 配置参数
 * @returns {string} robots.txt内容
 */
export function generateEnhancedRobotsTxt(props) {
  const { siteInfo, NOTION_CONFIG, options = {} } = props
  
  const LINK = siteInfo?.link || siteConfig('LINK')
  const AUTHOR = siteConfig('AUTHOR', 'Anonymous', NOTION_CONFIG)
  const TITLE = siteInfo?.title || siteConfig('TITLE')
  
  // 配置选项
  const {
    crawlDelay = 1,
    allowPaths = ['/'],
    disallowPaths = ['/api/', '/admin/', '/_next/', '/dashboard/', '/auth/'],
    customUserAgents = {},
    enableSitemapIndex = true,
    enableImageSitemap = true,
    enableNewsSitemap = false,
    additionalSitemaps = [],
    comments = true
  } = options
  
  let content = ''
  
  // 添加注释头部
  if (comments) {
    content += `# Robots.txt for ${TITLE}\n`
    content += `# Generated by NotionNext SEO Enhanced\n`
    content += `# Author: ${AUTHOR}\n`
    content += `# Generated on: ${new Date().toISOString().split('T')[0]}\n`
    content += `# Website: ${LINK}\n\n`
  }
  
  // 通用爬虫规则
  content += `# General crawlers\n`
  content += `User-agent: *\n`
  
  // 允许的路径
  allowPaths.forEach(path => {
    content += `Allow: ${path}\n`
  })
  
  // 禁止的路径
  disallowPaths.forEach(path => {
    content += `Disallow: ${path}\n`
  })
  
  // 爬取延迟
  if (crawlDelay > 0) {
    content += `Crawl-delay: ${crawlDelay}\n`
  }
  
  content += '\n'
  
  // Google特殊规则
  content += `# Google specific rules\n`
  content += `User-agent: Googlebot\n`
  content += `Allow: /\n`
  content += `Disallow: /api/\n`
  content += `Disallow: /admin/\n`
  
  // Google图片爬虫
  content += `\nUser-agent: Googlebot-Image\n`
  content += `Allow: /\n`
  content += `Disallow: /admin/\n`
  
  content += '\n'
  
  // 百度爬虫规则
  content += `# Baidu crawler rules\n`
  content += `User-agent: Baiduspider\n`
  content += `Allow: /\n`
  content += `Disallow: /api/\n`
  content += `Disallow: /admin/\n`
  content += `Crawl-delay: ${Math.max(crawlDelay, 1)}\n\n`
  
  // 必应爬虫规则
  content += `# Bing crawler rules\n`
  content += `User-agent: bingbot\n`
  content += `Allow: /\n`
  content += `Disallow: /api/\n`
  content += `Disallow: /admin/\n`
  content += `Crawl-delay: ${crawlDelay}\n\n`
  
  // 社交媒体爬虫
  content += `# Social media crawlers\n`
  content += `User-agent: facebookexternalhit\n`
  content += `Allow: /\n`
  content += `User-agent: Twitterbot\n`
  content += `Allow: /\n`
  content += `User-agent: LinkedInBot\n`
  content += `Allow: /\n\n`
  
  // 恶意爬虫屏蔽
  content += `# Block malicious crawlers\n`
  const maliciousBots = [
    'SemrushBot', 'AhrefsBot', 'MJ12bot', 'DotBot', 'BLEXBot'
  ]
  
  maliciousBots.forEach(bot => {
    content += `User-agent: ${bot}\n`
    content += `Disallow: /\n\n`
  })
  
  // 网站信息
  content += `# Website information\n`
  content += `Host: ${LINK}\n\n`
  
  // Sitemap信息
  content += `# Sitemaps\n`
  
  if (enableSitemapIndex) {
    content += `Sitemap: ${LINK}/sitemap.xml\n`
  }
  
  if (enableImageSitemap) {
    content += `Sitemap: ${LINK}/sitemap-images.xml\n`
  }
  
  // RSS订阅
  content += `Sitemap: ${LINK}/rss/feed.xml\n`
  
  // 额外的sitemap
  additionalSitemaps.forEach(sitemap => {
    content += `Sitemap: ${LINK}${sitemap}\n`
  })
  
  return content
}

/**
 * 写入robots.txt文件
 * @param {Object} props 配置参数
 */
export function writeEnhancedRobotsTxt(props) {
  const content = generateEnhancedRobotsTxt(props)
  
  try {
    // 确保public目录存在
    fs.mkdirSync('./public', { recursive: true })
    
    // 写入robots.txt
    fs.writeFileSync('./public/robots.txt', content)
    
    console.log('✅ Enhanced robots.txt generated successfully')
    return true
  } catch (error) {
    console.warn('⚠️ Failed to write robots.txt:', error.message)
    return false
  }
}